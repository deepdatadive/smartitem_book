--- 
title: "SMRT"
author: "Chris Foster"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "This is a book describing smart items and a compilation of research which can support their use."
---

# Preface

This book is a **compilation** of different reseach pieces all compiled together into a single volume. The purpose of the volume is to provide a concise, research oriented view of the smart item and other accompanying item formats. Each chapter will be a different research topic and we will attempt to group similar research articles in close proximity to each other within the book.

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```


<!--chapter:end:index.Rmd-->

# Introduction to Smart Items


Dave Really REally wants to be able to make a change to this text document.

A SmartItem is first and foremost an item, used on exams to measure important skills. Like traditional items, it has an ID number, is stored on a computer, is evaluated like traditional items with expert reviews, and eventually response data from examinees. It can be used on any kind of test design, such as CAT, LOFT, etc. If and when it doesnâ€™t function well, it can be repaired or deleted or retired.

The main difference between a smart item and a normal item is that the smart item is written with three areas of expertise: Subject matter, item writing and programming. With a well written and specific objective, with the help of a programmer and enough content expertise it is possible to write a single item which can cover the entire range of an objective. With the help of a programmer an item writer can write a series of stems, options, correct responses and incorrect responses that can generate a large amount of potential item derivatives based on a single objective. This process creates an item that is less static than a single multiple choice item.

In order better understand a smart item it is best to start with an example. An illustrativec example would come from an elementary math test. A single objective might be: Add two single digit numbers. There are only 10 single digit numbers (including 0) So really there is only (10!/ 2!(10-2)!) = 45 possible options as long as order doesn't matter.

Now, a single item writer could write all 45 items and cover the objective completely. However, it is also possible to write a simple program which generates all 45 possible questions. Now, for a fixed form test it would be likely that the item writer woudl not write all 45 options but instead write 2 or 3 of which one would be selected for the first form of the test while a different one might be selected for a second form. However, when administering a smart item to participants each participant would get a random stem and random options (including the correct option).

Now, for a simple math objective it might not be necessary to write an algorithm that writes the 45 different possible stems for the objective. However, imagine an objective where there are 206 possible answers such as "Identify each bone in the human body." Or perhaps there is an objective which asks participants to arrange 4 words in alphabetical order. The words can be anything in the human dictionary. Now there are 170,000 words in the english language and picking 4 leaves 3.479x10^19 possible options to completely cover the objective content and no item writer can write all of them and given current test construction methods there is no reason to do so.

## Purpose of Building Smart Items

## Smart Item Logic

A first step to understanding the logic behind smart items is to understand the logic of randomization in experimentation. Sir Ronald Fisher @fisher1925 outlined what is considered the cornerstone of experimental research today: randomization. Randomization has three primary purposes: 

1) It helps to distribute idosyncratic characteristics of participants to groups so that it does not bias the outcome. If participants could self select groups or were grouped based on characteristics than it could create systematic biases in the outcome based on participant characteristics.

2) Randomization helps calculate unbiased estimate of error effects. IE: those effects not attributable to the manipulation of an independent variable

3) Randomization helps ensure that error efects are statistically independent.

Now, considering point #1 a bit more: Randomization helps ensure that within group variability is Independent and identically distributed (IID) or in other words, within group variability does not contain bias and is simply noise. Without randomization it could easily contain any number of biases which could decrease or increase the differnces between groups. It is impossible to list all possible systematic biases that could creep into an experiment. Maybe all college educated participants self select themselves into a specific group or one gender reacts differently to a group assignment than another.

While other papers have talked in length about the importance of randomization in experimental design for the purposes of this section randomization removes systematic bias within group.

One natural artifact of the randomization process is an increase in within-group variation. If participants are asigned to groups based on characterisitcs or allowed to self select, more similar participants will end up in the same group reducing the amount of variability in the group. While a decrease in within-group variability inevitibly increases the probability of a significant effect in an experiment, the significant effect may simply be due to a bias brought by the selection process... which simply shows the importance of randomization. Even though variation is introduced, results are more trustworthy.









<!--chapter:end:02-Smart-Item-Intro.Rmd-->

# DOMC Difficulty Variance

## Initial Run
Here is a document showing the results of item families derived from a single DOMC stem. Essentially we treat each possible combination of options as a differential question just to see the amount of variance from a single DOMC stem.

In this first run we treat each different option combination as a different item, including those for people who never saw the correct response (making all these p-values 0)

```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
library(dplyr)
library(readr)
library(knitr)
library(ggplot2)
library(lemon)
library(stringr)

setwd_thisdir <- function () {
  this.dir <- dirname(parent.frame(3)$ofile)
  setwd(this.dir)
}

hp_data = read_csv('data/domc_order_difficulty/Full Responses After Exclusions.csv')

hp_clean = hp_data %>% filter(item_type == 'domc', item_component_type == 'domc_option') %>% group_by(delivery_id, item_id) %>% mutate(option_order = paste0(option_presented, collapse = ""))

hp_clean_mc = hp_data %>% filter(item_type == 'multiple_choice', item_component_type == 'final') %>% group_by(delivery_id, item_id) %>% mutate(option_order = paste0(option_presented, collapse = ""))
hp_clean_mc$survey = ifelse(grepl("Survey",hp_clean_mc$item_id),1,0)
hp_clean_mc = hp_clean_mc %>% filter(survey == 0)
hp_clean_mc$item_number = as.numeric(str_extract(hp_clean_mc$item_id, "[0-9]+"))

hp_summary = hp_clean %>% group_by(delivery_id, item_id) %>% summarize(item_total_seconds = max(item_total_seconds), item_order = max(option_order), score = max(score)) %>% group_by(delivery_id, item_id) %>% mutate(order = paste(sort(unlist(strsplit(as.character(item_order), ""))), collapse = "")) 

hp_summary_mc = hp_clean_mc %>% group_by(delivery_id, item_id) %>% summarize(item_total_seconds = max(item_total_seconds), item_order = max(option_order), score = max(score)) %>% group_by(delivery_id, item_id) %>% mutate(order = paste(sort(unlist(strsplit(as.character(item_order), ""))), collapse = "")) 

hp_items = hp_summary %>% group_by(item_id, order) %>% summarize(p_value = mean(score), count = n())
hp_items_mc = hp_summary_mc %>% group_by(item_id, item_order) %>% summarize(p_value = mean(score), count = n())

all_items = bind_rows(hp_items, hp_items_mc)
all_items$item_type = ifelse(is.na(all_items$item_order), 'DOMC', "MC")
all_items$item_number = as.numeric(str_extract(all_items$item_id, "[0-9]+"))

```

## Item 10B_v1

```{r, message=FALSE, warning=FALSE}
tenb_v1 = hp_items %>% filter(item_id == '10B_v1')
kable(tenb_v1)
```

### Histogram
```{r results='hide', message=FALSE, warning=FALSE}
theme_update(plot.title = element_text(hjust = 0.5))
ggplot(tenb_v1, aes(x=p_value)) + geom_histogram(binwidth=.01) + labs(title = "Item10B_v1") 
```
## Item 10A_v1

```{r, message=FALSE, warning=FALSE}
tena_v1 = hp_items_mc %>% filter(item_id == '10A_v1')
kable(tena_v1)
```
### Histogram
```{r results='hide', message=FALSE, warning=FALSE}
theme_update(plot.title = element_text(hjust = 0.5))
ggplot(tena_v1, aes(x=p_value)) + geom_histogram(binwidth=.01) + labs(title = "Item10A_v1") 
```

## Item 14b_v1

```{r, message=FALSE, warning=FALSE}
fourteenb_v1 = hp_items %>% filter(item_id == '14B_v1')
kable(fourteenb_v1, format = "markdown")
```
### Histogram

```{r results='hide', message=FALSE, warning=FALSE}
ggplot(fourteenb_v1, aes(x=p_value)) + geom_histogram(binwidth=.01) + labs(title = "Item14B_v1")+ xlim(-.01,1) + ylim(0,20)
```

## Item 14a_v1

```{r, message=FALSE, warning=FALSE}
fourteenb_v1 = hp_items_mc %>% filter(item_id == '14A_v1')
kable(fourteenb_v1, format = "markdown")
```
### Histogram

```{r results='hide', message=FALSE, warning=FALSE}
ggplot(fourteenb_v1, aes(x=p_value)) + geom_histogram(binwidth=.01) + labs(title = "Item14A_v1")+ xlim(-.01,1) + ylim(0,20)
```

## Remove correct response

We notice quickly that it is pretty difficult to interpret the p-values given that there is a significant amount of peole who never see the correct answer so the p-value for those "children" is 0. Here is what happens when we redefine what it means to be a different item. Instead of looking at every combination of seen options we are only going to look at distractors. This means that someone who saw the correct option and the first option (coded 01) will have "seen" the "same" question as someone who just saw the first distractor(coded 1).

It cleans things up a bit but it is important to recognize what is going on.



```{r results='hide', message=FALSE, warning=FALSE}

hp_summary$order_no_cr = gsub('0','',hp_summary$order)
hp_summary$order_no_cr = ifelse(hp_summary$order_no_cr == '', '0', hp_summary$order_no_cr)
hp_items_no_cr = hp_summary %>% group_by(item_id, order_no_cr) %>% summarize(p_value = mean(score), count = n())

all_items = bind_rows(hp_items_no_cr, hp_items_mc)
all_items$item_type = ifelse(is.na(all_items$item_order), 'DOMC', "MC")
all_items$item_number = as.numeric(str_extract(all_items$item_id, "[0-9]+"))


```

## Item 10B_v1

```{r, message=FALSE, warning=FALSE}
tenb_v1 = hp_items_no_cr %>% filter(item_id == '10B_v1')
kable(tenb_v1)
```

### Histogram

```{r, message=FALSE, warning=FALSE}
theme_update(plot.title = element_text(hjust = 0.5))
ggplot(tenb_v1, aes(x=p_value)) + geom_histogram(binwidth=.01) + labs(title = "Item10B_v1") + xlim(-.01,1)
```

## Item 14b_v1

```{r, message=FALSE, warning=FALSE}
fourteenb_v1 = hp_items_no_cr %>% filter(item_id == '14B_v1')
kable(fourteenb_v1)
```

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
ggplot(all_items %>% filter(item_number == 14), aes(x=p_value, color=item_type)) + geom_histogram(fill='white', binwidth=.01, position='identity', alpha=0) + labs(title = "Item14") + xlim(-.01,1) + geom_density()
```

### Histogram

```{r results='hide', message=FALSE, warning=FALSE}
ggplot(fourteenb_v1, aes(x=p_value)) + geom_histogram(binwidth=.01) + labs(title = "Item14b_v1") + xlim(-.01,1)
```

## All Item Plots

```{r, fig.height = 15, fig.width = 7, results='hide', message=FALSE, warning=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=20) 

ggplot(all_items, aes(x=p_value, color=item_type)) + geom_histogram(fill='white', binwidth=.01, position='identity', alpha=0) + xlim(-.01,1) + geom_density() + labs(title = "All Items") + xlim(-.01,1) +ylim(0,20) + facet_wrap(~item_number, ncol=3)

```




<!--chapter:end:03-DOMCDifficultyVariance.Rmd-->

# DOMC Difficulty Factors

The purpose of this chapter is to briefly address portions of the item format that may lead to systematic differences in difficulty for DOMC items

```{r}

library(dplyr)
library(readr)
library(knitr)
library(ggplot2)
library(lemon)

setwd_thisdir <- function () {
  this.dir <- dirname(parent.frame(3)$ofile)
  setwd(this.dir)
}


hp_data = read_csv('data/domc_order_difficulty/Full Responses After Exclusions.csv')

hp_clean = hp_data %>% filter(item_type == 'domc' | item_type == 'multiple_choice', item_component_type == 'domc_option' | item_component_type == 'final', !grepl('Survey', hp_data$item_id)) %>% group_by(delivery_id, item_id) %>% mutate(option_order = paste0(option_presented, collapse = ""))

hp_summary = hp_clean %>% group_by(delivery_id, item_id, item_type) %>% summarize(item_total_seconds = max(item_total_seconds), item_order = max(option_order), score = max(score)) %>% group_by(delivery_id, item_id) %>% mutate(order = paste(sort(unlist(strsplit(as.character(item_order), ""))), collapse = "")) %>% ungroup()

total_score = hp_summary %>% group_by(delivery_id) %>% summarize(total_score = sum(score))

hp_final = left_join(hp_summary, total_score)
hp_final$char_count = nchar(hp_final$order)

```

# Total Test Score and Options Seen

Simply running a logistic regression predicting if a person will get an item correct based on teh total number of options seen seems disingenous for a DOMC item. The reason being is that more proficient examinees will on average see more options as they are more likely to reject incorrect options. 


Here we look at some regression results

```{r}
mylogit <- glm(score ~ total_score + factor(item_type) + char_count, data = hp_final, family = "binomial")
summary(mylogit) 

mylogit <- glm(score ~ factor(item_type) + char_count + factor(item_type)*char_count, data = hp_final, family = "binomial")
summary(mylogit) 
```


<!--chapter:end:04-DOMCDifficultyFactor.Rmd-->

# Recall VS Deduction

In this chapter we will look at some differences between recall and deduction items on a Harry Potter assessment.

```{r cars}
library(dplyr)
library(readr)
library(knitr)
library(ggplot2)
library(lemon)
library(stringr)
setwd_thisdir <- function () {
  this.dir <- dirname(parent.frame(3)$ofile)
  setwd(this.dir)
}

hp_data = read_csv('data/domc_order_difficulty/Full Responses After Exclusions.csv')

hp_clean = hp_data %>% filter(item_type == 'domc' | item_type == 'multiple_choice', item_component_type == 'domc_option' | item_component_type == 'final', !grepl('Survey', hp_data$item_id)) %>% group_by(delivery_id, item_id) %>% mutate(option_order = paste0(option_presented, collapse = ""))

recall <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12")
deduction = c("13", "14", "15", "16", "17", "18", "19", "20", "21", "22")
hp_clean$deduction <- ifelse(grepl(paste(deduction,collapse="|"), hp_clean$item_id),1,0)

hp_items = hp_clean %>% group_by(item_id, item_type) %>% summarize(p_value = mean(score), count = n(), deduction = mean(deduction,na.rm=TRUE))
hp_items$item_number = str_extract(hp_items$item_id, "[0-9]+")
```

## Some Graphs

Here is the difference in p-value between DOMC and multiple choice for the same items.

```{r}

#here
ggplot(hp_items, aes(x=item_type, y=p_value, fill=item_type)) + geom_bar(stat="identity")  + facet_wrap(~item_number)

```

Here is the difference in p-value between DOMC and multiple choice for the same items.

```{r}
just_mc = hp_items %>% filter(item_type == 'multiple_choice')
just_domc = hp_items %>% filter(item_type != 'multiple_choice')

differences = hp_items %>% group_by(item_type) %>% summarize(average_p_value = mean(p_value, na.rm=TRUE))
kable(differences)

```

The total difference in p-values between item_types is `r mean(just_mc$p_value) - mean(just_domc$p_value)`.

Multiple Choice items appear to be slightly more difficult.

<!--chapter:end:05-recall_vs_deduction.Rmd-->

`r if (knitr::is_html_output()) '
#whocares
# References {-}
'`

<!--chapter:end:06-references.Rmd-->

# Simulation 1

In this chapter we will be talking about different simualtions run to address potential issues with smart items. To begin we are going to look at how test composition and difficulty change when people are administered random items from an item pool instead of a fixed form test.


## Generating Data

We generated a bank of 5000 items using the 2 prameter logistic model. The item difficulties were sampled from a normal distribution with a mean 0 and standard deviation of 1. The discrimination parameter was sampled from a normal distribution with mean 1 and a standard deviation of .05.

We then generated 100,000 examinees and had each examinee take tests comprised of random items from length 10 to 60 In addition to 50 random forms, each examinee took a fixed form test comprised of the same 40 items for each of the 100,000 examinees.

## Test Differences

It was important to come up with a way to determine if the random form was of differing difficulty than the 40 item fixed form test. How we did this was by computing the standard error of test difficulty using the p-values for items on the fixed form test. This was used to compute a confidence interval for the test difficulty. The standard error was st_dev/sqrt(test_length) = .028. Using a 95% confidence interval we see that the confidence interval for the difficulty of hte fixed form test is from .44 to .55. Now, whenever a candidate took a random form test we compared the difficulty of that test to the confidence interval above. If the difficulty of their test fell within the confidence interval we decided that this meant the random form test was no different in diffculty from the fixed form test.

There were several different ways of determining if a person received tests of different difficulties. The above method is simply one of the different ways of doing so.

Finally, we computed the percent of individuals at each item count that had a similar or different difficulty test. As we can see in the following table. When the random test was only 10 items long, 62% of candidates had a test that was either harder or easier than the 40 item fixed test. However, by the time the random test was 40 items long only 2% of people had a test that was either harder or easier than the fixed form test. As the number of items on the random test increased, the probability of it being a different diffculty test decreased. 

```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(dplyr)
library(knitr)
library(readr)

setwd_thisdir <- function () {
  this.dir <- dirname(parent.frame(3)$ofile)
  setwd(this.dir)
}

print(getwd())
sim_data1 = read_csv("data/simulation_data/simulation1_results.csv")

item_count_group = sim_data1 %>% group_by(item_count) %>% summarize(test_change_difficulty = 1 - mean(test_change_difficulty, na.rm=TRUE), count = n())
item_count_group$test_change_difficulty = item_count_group$test_change_difficulty*2

sim_data2 = sim_data1
sim_data2$test_change_difficulty = ifelse(sim_data2$test_change_difficulty == 1, 0, 1)

item_class_change = sim_data2 %>% group_by(item_count, test_change_difficulty, decision_change) %>% summarize(count = n())  %>% ungroup()
item_class_change = item_class_change %>% group_by(item_count)%>% mutate(freq = count / sum(count))

#item_count_change = sim_data1 %>% group_by(item_count, difficulty_change) %>% summarize(count =) 


```


```{r}
kable(item_count_group)
```


# Simulation 2

The first simulation showed that increasing the number of items on the random test resulted in more tests being equivilent in difficulty. However, one important part of many tests is evaluating what happens with the cut score decision. Tests arent 100% precise and their inprecision is captured in the reliability statistic. If a person took the test again tehy would not get the exact same score but hopefully get a similar score. However, the differences in the two different test scores may cross over the cut score decision so in one case they may pass and in another they may fail. When tests can also vary in difficulty it is possible that the change in the difficulty of hte test will result in larger numbers of people getting different cut score classifications because they recieved a different difficulty test.

In this second simulation we are going to expand on the data from the first simulation. We generated the data in the exact same way with one exception: we added in a modified angoff cut score to each of the 5000 items. This angoff cut score was generated by simply sampling a value from a normal distribution with a mean 15% easier than the item itself and a standard deviation of .05. So if an item had a p-value of .5, the cut score for the item would be sampled from a normal distribution from mean .65 and a standard deviation of .05. This made a test where the cut score was slightly lower than the average difficulty of the items on the test.

Each simulated examinee then took the random tests of varying length and the fixed form test and whether they passed each test based on the items in that test. If they got a different cut score decision on the fixed form test than the random test then there was a cut score classification change. We took extra care to look at cut score classification changes when an examinee passed the fixed test, recieved a harder random test and did not pass. We then evaluated the percent of people with classification changes and the precent of people with classification changes when given a harder random form test.

The results are reported in the table below. There are several important things to note in the table. 

1) The number of people who get a similar difficulty test but a different cut score classification is not zero.

2) The number of people who recieve a similar difficulty test but a different cut score classification goes down as the number of items increases.

3) The same is true for people with a different difficulty test. As the number of items increased, the number of people getting a different cut score classification and a different difficulty test shrinks

4) For around a random test around similar length as the fixed test only about .2% of people are recieving a different difficuly test AND also getting a different cut score classification. It is important to not that around 7% of people who got a simialr difficulty test got different cut score classifications. When considering unfairness on a test where a cut score decision is used only a very small percentage of people get both a different difficulty of test and a different cut score classification. 

```{r}
kable(item_class_change)
```

# Future Simulations

Obviously a simulation only works well if it the assumptions in data generation are reflective of reality and every simiulation has faults that can be addressed by adding in more information. Future simulations would include:

1) How is the cut score classification change impacted by different methods of generating angoff style cut scores for each item (for example, if the sample was not .15 easier than the item but instead was sample with a mean of 0 and sd .5)

2) How does the change in the discrimination parameter during the data generation process affect results. For example, if our items did not discriminate at 1 on average but instead .5?

3) What if there is a larger difference between the ability distribution and the item difficulty distribution?

4) What happens if instead of sampling items at random from the item pool you sample items whose scores correlate with eachother more than with the other items. This would be a better example of smart items where each item generated by smart item code would correlate more closely with other item generaged using the same code than every other item on the test.

<!--chapter:end:07-got_first_time.Rmd-->

