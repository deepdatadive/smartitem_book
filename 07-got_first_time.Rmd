# Simulation 1

In this chapter we will be talking about different simualtions run to address potential issues with smart items. To begin we are going to look at how test composition and difficulty change when people are administered random items from an item pool instead of a fixed form test.


## Generating Data

We generated a bank of 5000 items using the 2 prameter logistic model. The item difficulties were sampled from a normal distribution with a mean 0 and standard deviation of 1. The discrimination parameter was sampled from a normal distribution with mean 1 and a standard deviation of .05.

We then generated 100,000 examinees and had each examinee take tests comprised of random items from length 10 to 60 In addition to 50 random forms, each examinee took a fixed form test comprised of the same 40 items for each of the 100,000 examinees.

## Test Differences

It was important to come up with a way to determine if the random form was of differing difficulty than the 40 item fixed form test. How we did this was by computing the standard error of test difficulty using the p-values for items on the fixed form test. This was used to compute a confidence interval for the test difficulty. The standard error was st_dev/sqrt(test_length) = .028. Using a 95% confidence interval we see that the confidence interval for the difficulty of hte fixed form test is from .44 to .55. Now, whenever a candidate took a random form test we compared the difficulty of that test to the confidence interval above. If the difficulty of their test fell within the confidence interval we decided that this meant the random form test was no different in diffculty from the fixed form test.

There were several different ways of determining if a person received tests of different difficulties. The above method is simply one of the different ways of doing so.

Finally, we computed the percent of individuals at each item count that had a similar or different difficulty test. As we can see in the following table. When the random test was only 10 items long, 62% of candidates had a test that was either harder or easier than the 40 item fixed test. However, by the time the random test was 40 items long only 2% of people had a test that was either harder or easier than the fixed form test. As the number of items on the random test increased, the probability of it being a different diffculty test decreased. 

```{r results='hide', message=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(dplyr)
library(knitr)
library(readr)

setwd_thisdir <- function () {
  this.dir <- dirname(parent.frame(3)$ofile)
  setwd(this.dir)
}

print(getwd())
sim_data1 = read_csv("data/simulation_data/simulation1_results.csv")

item_count_group = sim_data1 %>% group_by(item_count) %>% summarize(test_change_difficulty = 1 - mean(test_change_difficulty, na.rm=TRUE), count = n())
item_count_group$test_change_difficulty = item_count_group$test_change_difficulty*2

sim_data2 = sim_data1
sim_data2$test_change_difficulty = ifelse(sim_data2$test_change_difficulty == 1, 0, 1)

item_class_change = sim_data2 %>% group_by(item_count, test_change_difficulty, decision_change) %>% summarize(count = n())  %>% ungroup()
item_class_change = item_class_change %>% group_by(item_count)%>% mutate(freq = count / sum(count))

#item_count_change = sim_data1 %>% group_by(item_count, difficulty_change) %>% summarize(count =) 


```


```{r}
kable(item_count_group)
```


# Simulation 2

The first simulation showed that increasing the number of items on the random test resulted in more tests being equivilent in difficulty. However, one important part of many tests is evaluating what happens with the cut score decision. Tests arent 100% precise and their inprecision is captured in the reliability statistic. If a person took the test again tehy would not get the exact same score but hopefully get a similar score. However, the differences in the two different test scores may cross over the cut score decision so in one case they may pass and in another they may fail. When tests can also vary in difficulty it is possible that the change in the difficulty of hte test will result in larger numbers of people getting different cut score classifications because they recieved a different difficulty test.

In this second simulation we are going to expand on the data from the first simulation. We generated the data in the exact same way with one exception: we added in a modified angoff cut score to each of the 5000 items. This angoff cut score was generated by simply sampling a value from a normal distribution with a mean 15% easier than the item itself and a standard deviation of .05. So if an item had a p-value of .5, the cut score for the item would be sampled from a normal distribution from mean .65 and a standard deviation of .05. This made a test where the cut score was slightly lower than the average difficulty of the items on the test.

Each simulated examinee then took the random tests of varying length and the fixed form test and whether they passed each test based on the items in that test. If they got a different cut score decision on the fixed form test than the random test then there was a cut score classification change. We took extra care to look at cut score classification changes when an examinee passed the fixed test, recieved a harder random test and did not pass. We then evaluated the percent of people with classification changes and the precent of people with classification changes when given a harder random form test.

The results are reported in the table below. There are several important things to note in the table. 

1) The number of people who get a similar difficulty test but a different cut score classification is not zero.

2) The number of people who recieve a similar difficulty test but a different cut score classification goes down as the number of items increases.

3) The same is true for people with a different difficulty test. As the number of items increased, the number of people getting a different cut score classification and a different difficulty test shrinks

4) For around a random test around similar length as the fixed test only about .2% of people are recieving a different difficuly test AND also getting a different cut score classification. It is important to not that around 7% of people who got a simialr difficulty test got different cut score classifications. When considering unfairness on a test where a cut score decision is used only a very small percentage of people get both a different difficulty of test and a different cut score classification. 

```{r}
kable(item_class_change)
```

# Future Simulations

Obviously a simulation only works well if it the assumptions in data generation are reflective of reality and every simiulation has faults that can be addressed by adding in more information. Future simulations would include:

1) How is the cut score classification change impacted by different methods of generating angoff style cut scores for each item (for example, if the sample was not .15 easier than the item but instead was sample with a mean of 0 and sd .5)

2) How does the change in the discrimination parameter during the data generation process affect results. For example, if our items did not discriminate at 1 on average but instead .5?

3) What if there is a larger difference between the ability distribution and the item difficulty distribution?

4) What happens if instead of sampling items at random from the item pool you sample items whose scores correlate with eachother more than with the other items. This would be a better example of smart items where each item generated by smart item code would correlate more closely with other item generaged using the same code than every other item on the test.
