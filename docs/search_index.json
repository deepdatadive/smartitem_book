[
["simulation-1.html", "7 Simulation 1 7.1 Generating Data 7.2 Test Differences", " 7 Simulation 1 In this chapter we will be talking about different simualtions run to address potential issues with smart items. To begin we are going to look at how test composition and difficulty change when people are administered random items from an item pool instead of a fixed form test. 7.1 Generating Data We generated a bank of 5000 items using the 2 prameter logistic model. The item difficulties were sampled from a normal distribution with a mean 0 and standard deviation of 1. The discrimination parameter was sampled from a normal distribution with mean 1 and a standard deviation of .05. We then generated 100,000 examinees and had each examinee take tests comprised of random items from length 10 to 60 In addition to 50 random forms, each examinee took a fixed form test comprised of the same 40 items for each of the 100,000 examinees. 7.2 Test Differences It was important to come up with a way to determine if the random form was of differing difficulty than the 40 item fixed form test. How we did this was by computing the standard error of test difficulty using the p-values for items on the fixed form test. This was used to compute a confidence interval for the test difficulty. The standard error was st_dev/sqrt(test_length) = .028. Using a 95% confidence interval we see that the confidence interval for the difficulty of hte fixed form test is from .44 to .55. Now, whenever a candidate took a random form test we compared the difficulty of that test to the confidence interval above. If the difficulty of their test fell within the confidence interval we decided that this meant the random form test was no different in diffculty from the fixed form test. There were several different ways of determining if a person received tests of different difficulties. The above method is simply one of the different ways of doing so. Finally, we computed the percent of individuals at each item count that had a similar or different difficulty test. As we can see in the following table. When the random test was only 10 items long, 62% of candidates had a test that was either harder or easier than the 40 item fixed test. However, by the time the random test was 40 items long only 2% of people had a test that was either harder or easier than the fixed form test. As the number of items on the random test increased, the probability of it being a different diffculty test decreased. kable(item_count_group) item_count test_change_difficulty count 10 0.6226226 999 11 0.5485485 999 12 0.5545546 999 13 0.4624625 999 14 0.4444444 999 15 0.3783784 999 16 0.3083083 999 17 0.3523524 999 18 0.3143143 999 19 0.3103103 999 20 0.2822823 999 21 0.2302302 999 22 0.2122122 999 23 0.2262262 999 24 0.1461461 999 25 0.1821822 999 26 0.1361361 999 27 0.1481481 999 28 0.1201201 999 29 0.1001001 999 30 0.0980981 999 31 0.0820821 999 32 0.0700701 999 33 0.0740741 999 34 0.0520521 999 35 0.0680681 999 36 0.0740741 999 37 0.0500501 999 38 0.0460460 999 39 0.0260260 999 40 0.0200200 999 41 0.0280280 999 42 0.0280280 999 43 0.0260260 999 44 0.0200200 999 45 0.0180180 999 46 0.0160160 999 47 0.0180180 999 48 0.0100100 999 49 0.0200200 999 50 0.0020020 999 51 0.0060060 999 52 0.0040040 999 53 0.0040040 999 54 0.0040040 999 55 0.0040040 999 56 0.0040040 999 57 0.0000000 999 58 0.0060060 999 59 0.0000000 999 "],
["simulation-2.html", "8 Simulation 2", " 8 Simulation 2 The first simulation showed that increasing the number of items on the random test resulted in more tests being equivilent in difficulty. However, one important part of many tests is evaluating what happens with the cut score decision. Tests arent 100% precise and their inprecision is captured in the reliability statistic. If a person took the test again tehy would not get the exact same score but hopefully get a similar score. However, the differences in the two different test scores may cross over the cut score decision so in one case they may pass and in another they may fail. When tests can also vary in difficulty it is possible that the change in the difficulty of hte test will result in larger numbers of people getting different cut score classifications because they recieved a different difficulty test. In this second simulation we are going to expand on the data from the first simulation. We generated the data in the exact same way with one exception: we added in a modified angoff cut score to each of the 5000 items. This angoff cut score was generated by simply sampling a value from a normal distribution with a mean 15% easier than the item itself and a standard deviation of .05. So if an item had a p-value of .5, the cut score for the item would be sampled from a normal distribution from mean .65 and a standard deviation of .05. This made a test where the cut score was slightly lower than the average difficulty of the items on the test. Each simulated examinee then took the random tests of varying length and the fixed form test and whether they passed each test based on the items in that test. If they got a different cut score decision on the fixed form test than the random test then there was a cut score classification change. We took extra care to look at cut score classification changes when an examinee passed the fixed test, recieved a harder random test and did not pass. We then evaluated the percent of people with classification changes and the precent of people with classification changes when given a harder random form test. The results are reported in the table below. There are several important things to note in the table. The number of people who get a similar difficulty test but a different cut score classification is not zero. The number of people who recieve a similar difficulty test but a different cut score classification goes down as the number of items increases. The same is true for people with a different difficulty test. As the number of items increased, the number of people getting a different cut score classification and a different difficulty test shrinks For around a random test around similar length as the fixed test only about .2% of people are recieving a different difficuly test AND also getting a different cut score classification. It is important to not that around 7% of people who got a simialr difficulty test got different cut score classifications. When considering unfairness on a test where a cut score decision is used only a very small percentage of people get both a different difficulty of test and a different cut score classification. kable(item_class_change) item_count test_change_difficulty decision_change count freq 10 0 0 570 0.5705706 10 0 1 118 0.1181181 10 1 0 252 0.2522523 10 1 1 59 0.0590591 11 0 0 617 0.6176176 11 0 1 108 0.1081081 11 1 0 215 0.2152152 11 1 1 59 0.0590591 12 0 0 612 0.6126126 12 0 1 110 0.1101101 12 1 0 218 0.2182182 12 1 1 59 0.0590591 13 0 0 663 0.6636637 13 0 1 105 0.1051051 13 1 0 191 0.1911912 13 1 1 40 0.0400400 14 0 0 661 0.6616617 14 0 1 116 0.1161161 14 1 0 179 0.1791792 14 1 1 43 0.0430430 15 0 0 700 0.7007007 15 0 1 110 0.1101101 15 1 0 156 0.1561562 15 1 1 33 0.0330330 16 0 0 741 0.7417417 16 0 1 104 0.1041041 16 1 0 131 0.1311311 16 1 1 23 0.0230230 17 0 0 740 0.7407407 17 0 1 83 0.0830831 17 1 0 153 0.1531532 17 1 1 23 0.0230230 18 0 0 750 0.7507508 18 0 1 92 0.0920921 18 1 0 135 0.1351351 18 1 1 22 0.0220220 19 0 0 755 0.7557558 19 0 1 89 0.0890891 19 1 0 129 0.1291291 19 1 1 26 0.0260260 20 0 0 760 0.7607608 20 0 1 98 0.0980981 20 1 0 124 0.1241241 20 1 1 17 0.0170170 21 0 0 780 0.7807808 21 0 1 104 0.1041041 21 1 0 99 0.0990991 21 1 1 16 0.0160160 22 0 0 782 0.7827828 22 0 1 111 0.1111111 22 1 0 92 0.0920921 22 1 1 14 0.0140140 23 0 0 782 0.7827828 23 0 1 104 0.1041041 23 1 0 95 0.0950951 23 1 1 18 0.0180180 24 0 0 832 0.8328328 24 0 1 94 0.0940941 24 1 0 64 0.0640641 24 1 1 9 0.0090090 25 0 0 807 0.8078078 25 0 1 101 0.1011011 25 1 0 79 0.0790791 25 1 1 12 0.0120120 26 0 0 836 0.8368368 26 0 1 95 0.0950951 26 1 0 56 0.0560561 26 1 1 12 0.0120120 27 0 0 833 0.8338338 27 0 1 92 0.0920921 27 1 0 63 0.0630631 27 1 1 11 0.0110110 28 0 0 859 0.8598599 28 0 1 80 0.0800801 28 1 0 45 0.0450450 28 1 1 15 0.0150150 29 0 0 869 0.8698699 29 0 1 80 0.0800801 29 1 0 46 0.0460460 29 1 1 4 0.0040040 30 0 0 851 0.8518519 30 0 1 99 0.0990991 30 1 0 40 0.0400400 30 1 1 9 0.0090090 31 0 0 862 0.8628629 31 0 1 96 0.0960961 31 1 0 40 0.0400400 31 1 1 1 0.0010010 32 0 0 887 0.8878879 32 0 1 77 0.0770771 32 1 0 29 0.0290290 32 1 1 6 0.0060060 33 0 0 879 0.8798799 33 0 1 83 0.0830831 33 1 0 34 0.0340340 33 1 1 3 0.0030030 34 0 0 882 0.8828829 34 0 1 91 0.0910911 34 1 0 25 0.0250250 34 1 1 1 0.0010010 35 0 0 877 0.8778779 35 0 1 88 0.0880881 35 1 0 29 0.0290290 35 1 1 5 0.0050050 36 0 0 882 0.8828829 36 0 1 80 0.0800801 36 1 0 30 0.0300300 36 1 1 7 0.0070070 37 0 0 902 0.9029029 37 0 1 72 0.0720721 37 1 0 22 0.0220220 37 1 1 3 0.0030030 38 0 0 879 0.8798799 38 0 1 97 0.0970971 38 1 0 23 0.0230230 39 0 0 900 0.9009009 39 0 1 86 0.0860861 39 1 0 13 0.0130130 40 0 0 909 0.9099099 40 0 1 80 0.0800801 40 1 0 10 0.0100100 41 0 0 908 0.9089089 41 0 1 77 0.0770771 41 1 0 14 0.0140140 42 0 0 907 0.9079079 42 0 1 78 0.0780781 42 1 0 13 0.0130130 42 1 1 1 0.0010010 43 0 0 901 0.9019019 43 0 1 85 0.0850851 43 1 0 11 0.0110110 43 1 1 2 0.0020020 44 0 0 913 0.9139139 44 0 1 76 0.0760761 44 1 0 8 0.0080080 44 1 1 2 0.0020020 45 0 0 914 0.9149149 45 0 1 76 0.0760761 45 1 0 8 0.0080080 45 1 1 1 0.0010010 46 0 0 915 0.9159159 46 0 1 76 0.0760761 46 1 0 7 0.0070070 46 1 1 1 0.0010010 47 0 0 923 0.9239239 47 0 1 67 0.0670671 47 1 0 6 0.0060060 47 1 1 3 0.0030030 48 0 0 903 0.9039039 48 0 1 91 0.0910911 48 1 0 5 0.0050050 49 0 0 916 0.9169169 49 0 1 73 0.0730731 49 1 0 9 0.0090090 49 1 1 1 0.0010010 50 0 0 915 0.9159159 50 0 1 83 0.0830831 50 1 0 1 0.0010010 51 0 0 920 0.9209209 51 0 1 76 0.0760761 51 1 0 3 0.0030030 52 0 0 920 0.9209209 52 0 1 77 0.0770771 52 1 0 2 0.0020020 53 0 0 916 0.9169169 53 0 1 81 0.0810811 53 1 0 2 0.0020020 54 0 0 932 0.9329329 54 0 1 65 0.0650651 54 1 0 2 0.0020020 55 0 0 941 0.9419419 55 0 1 56 0.0560561 55 1 0 2 0.0020020 56 0 0 940 0.9409409 56 0 1 57 0.0570571 56 1 0 2 0.0020020 57 0 0 927 0.9279279 57 0 1 72 0.0720721 58 0 0 943 0.9439439 58 0 1 53 0.0530531 58 1 0 3 0.0030030 59 0 0 922 0.9229229 59 0 1 77 0.0770771 "],
["future-simulations.html", "9 Future Simulations", " 9 Future Simulations Obviously a simulation only works well if it the assumptions in data generation are reflective of reality and every simiulation has faults that can be addressed by adding in more information. Future simulations would include: How is the cut score classification change impacted by different methods of generating angoff style cut scores for each item (for example, if the sample was not .15 easier than the item but instead was sample with a mean of 0 and sd .5) How does the change in the discrimination parameter during the data generation process affect results. For example, if our items did not discriminate at 1 on average but instead .5? What if there is a larger difference between the ability distribution and the item difficulty distribution? What happens if instead of sampling items at random from the item pool you sample items whose scores correlate with eachother more than with the other items. This would be a better example of smart items where each item generated by smart item code would correlate more closely with other item generaged using the same code than every other item on the test. "]
]
